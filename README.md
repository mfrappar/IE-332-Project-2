# IE-332-Project-2
Algorithm Ideas
- 
- 
- 
- 
- 
- 
- 






From online 
There are several types of adversarial attacks that could work on a binary image classifier. Some of the common adversarial attacks that can be used on binary image classifiers are:

-Fast Gradient Sign Method (FGSM): FGSM is a popular adversarial attack that uses the gradient of the classifier to generate adversarial examples. It adds a small perturbation to the input image in the direction of the gradient to maximize the classification error.
-Projected Gradient Descent (PGD): PGD is a stronger version of the FGSM attack. It iteratively applies the FGSM attack with a small step size and projection step to ensure that the generated adversarial example is close to the original input image.
-Carlini Wagner (CW) attack: The CW attack is a more advanced attack that uses an optimization algorithm to find the optimal perturbation that maximizes the classification error while minimizing the distortion in the generated adversarial example.
-Universal adversarial perturbation: Universal adversarial perturbations are small perturbations that can be added to any input image to cause misclassification by the classifier. The perturbation is generated by optimizing the perturbation vector to minimize the classification accuracy over a large set of input images.
-Spatial Transformation Attack: Spatial transformation attacks modify the input image by adding rotations, translations, and other spatial transformations to the image to cause misclassification by the classifier.
It's worth noting that each type of adversarial attack has its own strengths and weaknesses, and the choice of attack depends on the specific task and requirements of the binary image classifier.



